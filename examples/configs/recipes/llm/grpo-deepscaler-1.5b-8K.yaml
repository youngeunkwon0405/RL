defaults: ../../grpo_math_1B.yaml
grpo:
  num_prompts_per_step: 128
  num_generations_per_prompt: 8
  max_val_samples: 480
  val_batch_size: 32
loss_fn:
  reference_policy_kl_penalty: 0.0
checkpointing:
  keep_top_k: 10
  model_save_format: null
policy:
  model_name: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
  train_global_batch_size: 64
  train_micro_batch_size: 1
  max_total_sequence_length: 8192
  dtensor_cfg:
    cpu_offload: true
    sequence_parallel: true
    activation_checkpointing: true
    _v2: false
  sequence_packing:
    enabled: false
  optimizer:
    kwargs:
      lr: 2.0e-06
  generation:
    vllm_kwargs:
      compilation_config:
        use_inductor: false
data:
  dataset_name: DeepScaler
env:
  math:
    num_workers: 16
logger:
  monitor_gpus: false
cluster:
  gpus_per_node: 8
